{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zkb5rpD1XWe"
      },
      "source": [
        "\n",
        "# **Imports**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjRCZk9ihgI6",
        "outputId": "f6f3b044-cebe-4198-94fd-a3ca2fe2552c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import json\n",
        "import requests\n",
        "import threading\n",
        "import concurrent.futures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANDt02O-1egR"
      },
      "source": [
        "# **Collecting data**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tmWzwBGxhpSL"
      },
      "outputs": [],
      "source": [
        "total_elements = 2100000\n",
        "batch_size = 200\n",
        "start_index = 0\n",
        "all_elements = []\n",
        "lock = threading.Lock()\n",
        "\n",
        "def fetch_data(start_index):\n",
        "    url = f\"https://wasabi.i3s.unice.fr/api/v1/song_all/{start_index}\"\n",
        "    print(start_index,'-')\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    return None\n",
        "def process_result(result):\n",
        "    if result:\n",
        "        with lock:\n",
        "            all_elements.extend(result)\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    futures = [executor.submit(fetch_data, start_index + i * batch_size) for i in range(total_elements // batch_size)]\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        result = future.result()\n",
        "        process_result(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uP9xjTTpAmb"
      },
      "outputs": [],
      "source": [
        "output_file = 'results.json'\n",
        "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(all_elements, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f'Data saved to {output_file}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T7X2uY615oI"
      },
      "source": [
        "# **Loading data**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg3RFHhMnFSE"
      },
      "outputs": [],
      "source": [
        "file_path =  'results.json'\n",
        "data = pd.read_json(file_path)\n",
        "df_artists = pd.read_csv('wasabi_artists.csv')\n",
        "df_albums = pd.read_csv('wasabi_albums.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf1lMJeU2Auf"
      },
      "source": [
        "# **Data processing: Wasabi songs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRVMucFmfSz8"
      },
      "outputs": [],
      "source": [
        "# dropping useless columns\n",
        "data = data.drop(columns=['lyrics','title_accent_fold','urlAllmusic','urlAmazon','urlGoEar','urlHypeMachine','urlITunes','urlLastFm',\n",
        "                       'urlMusicBrainz','urlPandora','urlSong','urlWikipedia','id_song_musicbrainz','disambiguation','bpm','urlSpotify','explicitLyrics','abstract','format','animux_path','animux_content', 'animux_contents',\n",
        "                        'aligned_id','preview','begin','end', 'animux_paths', 'explicit_content_lyrics','chords_metadata','multitrack_path', 'animux_paths', 'explicit_content_lyrics',\n",
        "                        'chords_metadata' ,'subject','summary','genre'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laloG6sFpPyN"
      },
      "outputs": [],
      "source": [
        "# extracting the genre of a given title\n",
        "def map_title_to_genre(title):\n",
        "  return df_albums[df_albums.title == title].genre.values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4nML7NTpTLa"
      },
      "outputs": [],
      "source": [
        "title_to_genre = df_albums.set_index('title')['genre'].to_dict()\n",
        "data['genre'] = data['albumTitle'].map(title_to_genre).fillna('Unknown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWeM6gGdpiyN"
      },
      "outputs": [],
      "source": [
        "#dictionary where titles are keys, and 'id_artist' values from  'df_albums' are values\n",
        "title_to_artist= df_albums.set_index('title')['id_artist'].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKAswzekpqig"
      },
      "outputs": [],
      "source": [
        "# artist dict to extract information of the previous titles\n",
        "artists = {}\n",
        "for artist_info, id_artist in title_to_artist.items():\n",
        "    artist_name = df_artists[df_artists['_id'] == id_artist]['_id'].values[0]\n",
        "    artists[artist_name] = artist_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt5in_Qvps_7"
      },
      "outputs": [],
      "source": [
        "#processing album title sequence\n",
        "data['albumTitle'] = data['albumTitle'].str.strip().str.lower()\n",
        "\n",
        "# Reverse the dictionary (reverse album names  and artist )\n",
        "artists_to_albums = {v.lower(): k for k, v in artists.items()}\n",
        "\n",
        "# mapping album names to artist and store it in \"artist\" column in df_albums\n",
        "data['artist'] = data['albumTitle'].map(artists_to_albums)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aTedhHfp354"
      },
      "outputs": [],
      "source": [
        "#dropping nan values\n",
        "data.dropna(subset=['artist'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imdFqkaUp78l"
      },
      "outputs": [],
      "source": [
        "# create new dataframe with specific information\n",
        "result_df = pd.DataFrame(columns=['artist_id', 'name','type','genre','location'])\n",
        "#populating the df according to artists in data\n",
        "for artist_id in data.artist.values:\n",
        "    artist_location = df_artists[df_artists['_id'] == artist_id]['location'].values[0]\n",
        "    artist_name = df_artists[df_artists['_id'] == artist_id]['name'].values[0]\n",
        "    artist_type = df_artists[df_artists['_id'] == artist_id]['type'].values[0]\n",
        "    artist_genre= df_artists[df_artists['_id'] == artist_id]['genres'].values[0]\n",
        "    result_df = result_df.append({'artist_id': artist_id,'name':artist_name,'type':artist_type,'genre':artist_genre, 'location': artist_location}, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORXowCwYqBRF"
      },
      "outputs": [],
      "source": [
        "#get artist country\n",
        "result_df['country'] =  result_df['location'].apply(lambda x: json.loads(x)['country'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW3X_64JqDxl"
      },
      "outputs": [],
      "source": [
        "#updating data\n",
        "data['artist_country'] = result_df.country\n",
        "data['artist_name'] = result_df.name\n",
        "data['artist_type'] = result_df.type\n",
        "data['artist_genre'] = result_df.genre\n",
        "data = data.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMECWSdF7eP7"
      },
      "outputs": [],
      "source": [
        "new_df = pd.DataFrame()\n",
        "# grouping data by 'artist_name'\n",
        "grouped = data.groupby('artist_name')\n",
        "\n",
        "# for each artist\n",
        "for artist_name, group_data in grouped:\n",
        "    # get country, artist type, and genre\n",
        "    artist_country = group_data['artist_country'].iloc[0]\n",
        "    artist_type = group_data['artist_type'].iloc[0]\n",
        "    genre = group_data['genre'].iloc[0]\n",
        "\n",
        "    # get unique albums\n",
        "    albums_info = group_data.groupby(['id_album', 'albumTitle', 'publicationDateAlbum']) \\\n",
        "                            .agg({'isClassic': 'first'}) \\\n",
        "                            .reset_index() \\\n",
        "                            .to_dict(orient='records')\n",
        "\n",
        "    # get songs information\n",
        "    songs_info = group_data.apply(lambda row: {\n",
        "        'title': row['title'],\n",
        "        'releaseDate': row['publicationDateAlbum'],\n",
        "        'isClassic': row['isClassic'],\n",
        "        'urlYouTube': row['urlYouTube'],\n",
        "        'duration': row['length'],\n",
        "        'language_detect': row['language_detect'],\n",
        "        'runtime': row['runtime'],\n",
        "        'award': row['award'],\n",
        "        'producer': row['producer'],\n",
        "        'writer': row['writer'],\n",
        "        'DeezerURL': row['urlDeezer'],\n",
        "        'Album': row['albumTitle']\n",
        "    }, axis=1).tolist()\n",
        "\n",
        "    # dictionary for the artist's information\n",
        "    artist_info = {\n",
        "        'artist_name': artist_name,\n",
        "        'artist_type': artist_type,\n",
        "        'artist_genre': artist_genre,\n",
        "        'artist_country': artist_country,\n",
        "        'albums': albums_info,\n",
        "        'songs': songs_info\n",
        "    }\n",
        "    new_df = new_df.append(artist_info, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MByDz55k6Lm-"
      },
      "outputs": [],
      "source": [
        "mode_value = new_df[new_df['artist_genre'] != 'Unknown']['artist_genre'].mode()[0]\n",
        "new_df['artist_genre'].replace('Unknown', mode_value, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOhu-Y1L6NRV"
      },
      "outputs": [],
      "source": [
        "genres_to_check = ['Rock', 'Punk Rock', 'Alternative Rock', 'Doom Metal', 'Folk Och Rackare', 'Contemporary Christian', 'Heavy Metal', 'Pop Rock', 'Post-Hardcore','Folk Rock','Progressive Metal', 'Pagan Metal', 'Funk Metal', 'Indie Rock', 'Christian Metal']\n",
        "new_df.loc[new_df['artist_genre'].isin(genres_to_check), 'artist_type'] = 'Group'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8ZTky8z6NTz"
      },
      "outputs": [],
      "source": [
        "# Define the list of genres to consider\n",
        "genres_to_change = ['Pop', 'Chanson', 'Freestyle','Hip Hop']\n",
        "\n",
        "# Set 'artist_type' to 'Person' for the selected genres\n",
        "new_df.loc[new_df['artist_genre'].isin(genres_to_change), 'artist_type'] = 'Person'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDPeX_4E6NWo"
      },
      "outputs": [],
      "source": [
        "# List of artists confirmed as \"Person\"\n",
        "individual_artists = [\n",
        "    'Grace Kennedy',\n",
        "    'Janne Westerlund',\n",
        "    'Jean Carlos',\n",
        "    'Joel Adams',\n",
        "    'Jonas Blue',\n",
        "    'Clover',\n",
        "    'Jupiter',\n",
        "    'Y.C.',\n",
        "    'Divingstation',\n",
        "    'Juan Muteniac',\n",
        "    'Heinz Rennhack',\n",
        "    'InMemory',\n",
        "    'H-2-S',\n",
        "    'Exit (FI)'\n",
        "]\n",
        "\n",
        "# Set 'artist_type' to 'Person' for the confirmed individual artists\n",
        "new_df.loc[new_df['artist_name'].isin(individual_artists), 'artist_type'] = 'Person'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQ_JDFdI6NY8"
      },
      "outputs": [],
      "source": [
        "# List of artists confirmed as \"Group\"\n",
        "group_artists = [\n",
        "    'Communauté De Taizé',\n",
        "    'Harold Budd/Brian Eno',\n",
        "    'Freefonix',\n",
        "    'Fragil Vida',\n",
        "    'Jonks',\n",
        "\n",
        "]\n",
        "\n",
        "# Set 'artist_type' to 'Group' for the confirmed group artists\n",
        "new_df.loc[new_df['artist_name'].isin(group_artists), 'artist_type'] = 'Group'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2ENobSC6NbT"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Create a mapping between languages and countries (this is a simplified example)\n",
        "language_to_country = {\n",
        "    'english': ['United States', 'United Kingdom'],\n",
        "    'spanish': 'Spain',\n",
        "    'german': 'Germany',\n",
        "    'french': 'France',\n",
        "    'italian': 'Italy',\n",
        "    'portuguese': 'Portugal',\n",
        "    'polish': 'Poland',\n",
        "    'swedish': 'Sweden',\n",
        "    'hausa': 'Nigeria',\n",
        "    'dutch': 'Netherlands',\n",
        "    'norwegian': 'Norway',\n",
        "    'turkish': 'Turkey',\n",
        "    'finnish': 'Finland',\n",
        "    'indonesian': 'Indonesia',\n",
        "    'croatian': 'Croatia',\n",
        "    'hungarian': 'Hungary',\n",
        "    'romanian': 'Romania',\n",
        "    'lithuanian': 'Lithuania',\n",
        "    'swahili': 'Kenya',\n",
        "    'danish': 'Denmark',\n",
        "    'slovak': 'Slovakia',\n",
        "    'latin': 'Vatican City',\n",
        "    'somali': 'Somalia',\n",
        "    'welsh': 'Wales',\n",
        "    'cebuano': 'Philippines',\n",
        "    'hawaiian': 'Hawaii',\n",
        "    'tagalog': 'Philippines',\n",
        "    'estonian': 'Estonia',\n",
        "    'albanian': 'Albania',\n",
        "    'slovene': 'Slovenia',\n",
        "    'icelandic': 'Iceland',\n",
        "    'latvian': 'Latvia',\n",
        "    'vietnamese': 'Vietnam'\n",
        "}\n",
        "\n",
        "\n",
        "# Function to estimate the country based on song language\n",
        "def estimate_country(songs):\n",
        "    # Initialize a list to store the estimated countries\n",
        "    estimated_countries = []\n",
        "\n",
        "    # Iterate through the songs to extract languages\n",
        "    languages = set(song['language_detect'] for song in songs)\n",
        "\n",
        "    # Check if English is one of the languages\n",
        "    if 'english' in languages:\n",
        "        # Randomly choose between \"United States\" and \"United Kingdom\" with 50% probability each\n",
        "        chosen_country = random.choice(language_to_country['english'])\n",
        "        estimated_countries.append(chosen_country)\n",
        "    else:\n",
        "        # Use the mapping to estimate the country for other languages\n",
        "        for lang in languages:\n",
        "            estimated_country = language_to_country.get(lang, 'Unknown')\n",
        "            estimated_countries.append(estimated_country)\n",
        "\n",
        "    return estimated_countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK0VCk5O6Nd0"
      },
      "outputs": [],
      "source": [
        "new_df['artist_country'] = new_df['songs'].apply(estimate_country)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORDQv1u-6WW_"
      },
      "outputs": [],
      "source": [
        "new_df['artist_country'] = new_df['artist_country'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzz57lnw6Wfy"
      },
      "outputs": [],
      "source": [
        "# Create a mapping of artists to their countries\n",
        "artist_country_mapping = {\n",
        "    'Akcent': 'Romania',\n",
        "    'Clipse': 'United States',\n",
        "    'Coco Montoya': 'United States',\n",
        "    'Cold Chisel': 'Australia',\n",
        "    'Coles Whalen': 'United States',\n",
        "    'Dirty Projectors': 'United States',\n",
        "    'Disharmonic Orchestra': 'Austria',\n",
        "    'Diva': 'South Korea',\n",
        "    'Ed Motta': 'Brazil',\n",
        "    'Eiffel 65': 'Italy',\n",
        "    'Elend': 'France',\n",
        "    'Embrace The End': 'United States',\n",
        "    'Embrace Today': 'United States',\n",
        "    'Embraced (US)': 'United States',\n",
        "    'Enter My Silence': 'Sweden',\n",
        "    'Esterlyn': 'United States',\n",
        "    'Exist†trace': 'Japan',\n",
        "    'F(x)': 'South Korea',\n",
        "    'FM (CA)': 'United States',\n",
        "    'Fagner': 'Brazil',\n",
        "    'Fancy': 'Germany',\n",
        "    'Farewell, My Love': 'United States',\n",
        "    'Fauxliage': 'United States/Canada',\n",
        "    'Freeky Cleen': 'Ukraine',\n",
        "    'Fresh Body Shop': 'France',\n",
        "    'Gideon Emery': 'South Africa',\n",
        "    'Giovanca': 'Netherlands',\n",
        "    'Grandaddy': 'United States',\n",
        "    'Great Lake Swimmers': 'Canada',\n",
        "    'Greg Bates': 'United States',\n",
        "    'Hemlock': 'United States',\n",
        "    'Hexenhaus': 'Sweden',\n",
        "    'Iration': 'United States',\n",
        "    'Iuno': 'Germany',\n",
        "    'Ivan Graziani': 'Italy',\n",
        "    'JJ Grey & Mofro': 'United States',\n",
        "    'Jeremy Fisher': 'Canada',\n",
        "    'Joel Adams': 'Australia',\n",
        "    'Julie Covington': 'United Kingdom',\n",
        "    'Jumbo': 'Mexico',\n",
        "    'Junkyard': 'United States',\n",
        "    'Jupiter': 'Japan',\n",
        "    'Kal P. Dal': 'Sweden',\n",
        "    'Kanda, Kodža I Nebojša': 'Serbia',\n",
        "    'Kataklysm': 'Canada',\n",
        "    'Shawn Colvin': 'United States',\n",
        "    'Sleepy Sun': 'United States',\n",
        "    'A Tribute To Jens': 'Sweden',\n",
        "    'A Good Day For Killing': 'United States',\n",
        "    'Agent Simple':' Finlande' ,\n",
        "    'Cold Fusion & Rukkanor': 'Germany',\n",
        "    'Adrian Belew': 'United States',\n",
        "    'Agent 51': 'United States',\n",
        "    'Cirkus Miramar': 'Sweden',\n",
        "    'Comadre': 'United States',\n",
        "    'Dim Mak': 'United States',\n",
        "    'Dirty On Purpose': 'United States',\n",
        "    'Diva (DK)': 'Denmark',\n",
        "    'Empire Of The Sun': 'Australia',\n",
        "    'Exit (ES)': 'Spain',\n",
        "    'Faderhead': 'Germany',\n",
        "    'Far From Alaska': 'Brazil',\n",
        "    'Ferris MC': 'Germany',\n",
        "    'Flegmaatikot': 'Finland',\n",
        "    'Forever In Combat': 'United States',\n",
        "    'Fountains Of Wayne': 'United States',\n",
        "    'Frank Black': 'United States',\n",
        "    'Fredl Fesl': 'Austria',\n",
        "    'Fredrik Furu': 'Sweden',\n",
        "    'Fredrik Miller': 'Sweden',\n",
        "    \"Fredrik Thordendal's Special Defects\": 'Sweden',\n",
        "    'Fredrik Vahle': 'Germany',\n",
        "    'Friends (US)': 'United States',\n",
        "    'G. Dep': 'United States',\n",
        "    'Gibonni': 'Croatia',\n",
        "    'Grandmaster Flash And The Furious Five': 'United States',\n",
        "    'Greg Trooper': 'United States',\n",
        "    'Group 1 Crew': 'United States',\n",
        "    'Guy': 'United States',\n",
        "    'Gwenmars': 'United States',\n",
        "    'Gökhan Özen': 'Turkey',\n",
        "    'Göksel': 'Turkey',\n",
        "    'Halfway Home': 'United States',\n",
        "    'Halvdan Sivertsen': 'Norway',\n",
        "    'Hampton The Hampster': 'United States',\n",
        "    'Hollie Cook': 'United Kingdom',\n",
        "    'Hollow Haze': 'Italy',\n",
        "    'Héctor Acosta': 'Dominican Republic',\n",
        "    'Jackie DeShannon': 'United States',\n",
        "    'Jan Terri': 'United States',\n",
        "    'Jodarok': 'Finland',\n",
        "    'Jordan Smith': 'United States',\n",
        "    'José Carreras': 'Spain',\n",
        "    'Junior Kimbrough': 'United States',\n",
        "    'Justin Mauriello': 'United States',\n",
        "    'Kante': 'Germany',\n",
        "    'Love Charisse': 'United States',\n",
        "    'Northern Lights (UK)': 'United Kingdom',\n",
        "    'Sarah Solovay': 'United States',\n",
        "    'Communion Of Thieves': 'United States',\n",
        "    'Emmanuel Moire': 'France',\n",
        "    'Jorddy': 'France',\n",
        "    'Heinz Rennhack':'Poland',\n",
        "    'Heltah Skeltah': 'United States',\n",
        "    'Divine Souls': 'United States',\n",
        "    'Empirine': 'Sweden',\n",
        "    'Embracing': 'United Kingdom',\n",
        "    'G-Squad': 'France',\n",
        "    'GNR': 'United States',\n",
        "    'GTR':  'United Kingdom',\n",
        "    'Ghost Of A Fallen Age' :'United States',\n",
        "    'Her Nightmare': 'Australia',\n",
        "    'The Echoes':  'United Kingdom',\n",
        "\n",
        "}\n",
        "\n",
        "# Update the 'artist_country' column based on the mapping\n",
        "for artist, country in artist_country_mapping.items():\n",
        "    new_df.loc[new_df['artist_name'] == artist, 'artist_country'] = country\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMdhhlfl6Wjc"
      },
      "outputs": [],
      "source": [
        "new_df['artist_country'] = new_df['artist_country'].str.strip()\n",
        "new_df['artist_country'] = new_df['artist_country'].str.split(',').str[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZoB3itg6eGc"
      },
      "outputs": [],
      "source": [
        "data['albums'] = data['albums'].apply(lambda albums_list: [album if album['publicationDateAlbum'] != '' else {'id_album': album['id_album'], 'albumTitle': album['albumTitle'], 'publicationDateAlbum': '0000', 'isClassic': album['isClassic']} for album in albums_list])\n",
        "# Define a function to process the dictionaries within the 'songs' column\n",
        "def process_song(song):\n",
        "    keys_to_remove = ['duration', 'producer', 'writer']\n",
        "    song = {key: value for key, value in song.items() if key not in keys_to_remove}\n",
        "    if song.get('award') is None:\n",
        "        song['award'] = 'No award'\n",
        "    return song\n",
        "\n",
        "# Assuming 'new_df' is your DataFrame and 'songs' is the column with lists of dictionaries\n",
        "data['songs'] = data['songs'].apply(lambda songs_list: [process_song(song) for song in songs_list])\n",
        "\n",
        "\n",
        "# Function to update the 'album' and 'releaseDate' keys in each song dictionary\n",
        "def update_songs(song_list, album_list):\n",
        "    for song in song_list:\n",
        "        if 'Album' not in song:\n",
        "            song['Album'] = album_list[0]['albumTitle'] if album_list else None\n",
        "        if song.get('releaseDate', '').strip() == '':\n",
        "            song['releaseDate'] = album_list[0].get('publicationDateAlbum', '0000')\n",
        "        if 'runtime' not in song or song['runtime'] is None or song['runtime'] == '':\n",
        "            song['runtime'] = '60'\n",
        "        if 'urlYouTube' not in song or song['urlYouTube'] == '' or 'urlYouTube' ==None:\n",
        "            song['urlYouTube'] = 'https://www.youtube.com/'\n",
        "        if 'DeezerURL' not in song or song['DeezerURL'] == '' or song['DeezerURL'] is None:\n",
        "            song['DeezerURL'] = 'https://www.deezer.com/fr/'  # Set a default Deezer URL\n",
        "    return song_list\n",
        "# Apply the function to update the 'songs' column\n",
        "data['songs'] = data.apply(lambda row: update_songs(row['songs'], row['albums']), axis=1)\n",
        "\n",
        "data['songs'] = data['songs'].apply(lambda songs_list: [song if song['releaseDate'].isdigit() else {'title': song['title'], 'releaseDate': '0000', 'isClassic': song['isClassic']} for song in songs_list])\n",
        "\n",
        "# Function to calculate the difference between the most recent and oldest song years\n",
        "def calculate_activity_period(songs_list):\n",
        "    release_years = [int(song['releaseDate']) for song in songs_list if song['releaseDate'] != '0000']\n",
        "    if not release_years:\n",
        "        return 2\n",
        "    return max(release_years) - min(release_years)\n",
        "\n",
        "# Create a new column \"activity period\"\n",
        "data['activity_period'] = data['songs'].apply(calculate_activity_period)\n",
        "data['songs'] = data['songs'].apply(lambda songs_list: [song if song['releaseDate'].isdigit() else {'title': song['title'], 'releaseDate': '0000', 'isClassic': song['isClassic']} for song in songs_list])\n",
        "\n",
        "# Function to calculate the difference between the most recent and oldest song years\n",
        "def calculate_activity_period(songs_list):\n",
        "    release_years = [int(song['releaseDate']) for song in songs_list if song['releaseDate'] != '0000']\n",
        "    if not release_years:\n",
        "        return 1\n",
        "    return max(release_years) - min(release_years)\n",
        "\n",
        "# Create a new column \"activity period\"\n",
        "data['activity_period'] = data['songs'].apply(calculate_activity_period)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ68uJup6wDB"
      },
      "source": [
        "# **Data Processing: for Choropleth map**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vcc3kJN76e6t"
      },
      "outputs": [],
      "source": [
        "choro = pd.read_json('wasabi_songs.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhkd4m_a6_Fb"
      },
      "outputs": [],
      "source": [
        "df_albums['_id'] = [df_albums._id.values[i][9:-1] for i in range(len(df_albums))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rn_U8ER6_IN"
      },
      "outputs": [],
      "source": [
        "unique_countries = []\n",
        "l = [ np.unique(l) for l in choro.availableCountries.values]\n",
        "for ll in l:\n",
        "  unique_countries.extend(np.unique(ll))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSBSA6O16_Kg"
      },
      "outputs": [],
      "source": [
        "countries = np.unique(unique_countries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ9iB02A6_N9"
      },
      "outputs": [],
      "source": [
        "dico = {}\n",
        "\n",
        "with open('code_country.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "for line in lines[1:]:\n",
        "    elements = line.strip().split(':')[0].split(',')\n",
        "    dico[elements[1]] = elements[0]\n",
        "\n",
        "no = ['AN',  'BO',  'BQ',  'CD',  'FM',  'IR',  'KP',  'KR',  'MD',  'MK',  'PS',  'SH',  'TW',  'TZ',  'VE',  'VG',  'VI']\n",
        "y = [\"Netherlands Antilles \",\" Bolivia\",\" Bonaire\",\" Sint Eustatius\",\" Saba \",\" Democratic Republic of the Congo\",\" Federated States of Micronesia\",\" Iran\",\" North Korea \",\" South Korea \",\" Moldova\",\" North Macedonia (\",\" Palestinian Territories \",\" Saint Helena\",\" Ascension and Tristan da Cunha\",\" Taiwan \",\" Tanzania\",\" Venezuela\",\" British Virgin Islands\",\" United States Virgin Islands\"]\n",
        "for i in range(len(no)):\n",
        "  dico[no[i]] = y[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJLj0miA7E88"
      },
      "outputs": [],
      "source": [
        "def get_country(countries):\n",
        "  values_list = []\n",
        "  for key in countries:\n",
        "    if key in dico:\n",
        "      values_list.append(dico[key])\n",
        "    else:\n",
        "      values_list.append(key)\n",
        "  return values_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIMiWF2j7FAu"
      },
      "outputs": [],
      "source": [
        "coun = get_country(countries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsAeUpYg7FD6"
      },
      "outputs": [],
      "source": [
        "choro['countries'] = [get_country(cl) for cl in choro['availableCountries']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqPDGXAr7Jnb"
      },
      "outputs": [],
      "source": [
        "d = choro.groupby(['countries'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvIYff3M7KJj"
      },
      "outputs": [],
      "source": [
        "languages = np.unique(choro.language.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8ADshHg7KOi"
      },
      "outputs": [],
      "source": [
        "dico_languages = {'':'','dan':'danish','deu':'deutch','eng': 'english','fin':'finnish','fra':'french','ita':'italian','jpn':'japanese','ksh': 'kolsch','por': 'portuguese','spa': 'spanish','zxx':'zxx'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXPqmUDH7KUG"
      },
      "outputs": [],
      "source": [
        "choro['lang'] = [dico_languages[key] for key in choro['language']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW2b1fnq7KZX"
      },
      "outputs": [],
      "source": [
        "np.unique(choro[choro['lang'] !=choro['language_detect']][['lang','language_detect']].language_detect.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLrcqFfG7Rkb"
      },
      "outputs": [],
      "source": [
        "ids_artists = [df_artists._id.values[i][9:-1] for i in range(len(df_artists))]\n",
        "ids_artists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tma3HzmH7Rrz"
      },
      "outputs": [],
      "source": [
        "ids_alb =[df_albums._id.values[i][9:-1] for i in range(len(df_albums))]\n",
        "ids_alb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sTDds_l7Rwv"
      },
      "outputs": [],
      "source": [
        "albums_in_df = [choro['id_album'] == id for id in ids_alb]\n",
        "albums_in_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SdMI9f07Xvp"
      },
      "outputs": [],
      "source": [
        "new_choro = choro[[\"name\",\"countries\",\"lang\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6hn6KGB7Xyg"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(choro)\n",
        "\n",
        "# Create a list of unique countries\n",
        "unique_countries = set(country for sublist in df['countries'] if isinstance(sublist, list) for country in sublist)\n",
        "\n",
        "# Initialize an empty list to store the results\n",
        "result_list = []\n",
        "\n",
        "# Iterate through each unique country\n",
        "for country in unique_countries:\n",
        "    for index, row in df.iterrows():\n",
        "        if isinstance(row['countries'], list) and country in row['countries available']:\n",
        "            result_list.append([country, row['lang'], row['name']])\n",
        "\n",
        "# Create a new data frame from the result list\n",
        "result_df = pd.DataFrame(result_list, columns=['Country', 'Language', 'Song'])\n",
        "\n",
        "# Group the data by Country and Language and aggregate the song names\n",
        "result_grouped = result_df.groupby(['Country', 'Language'],group_keys=False)['Song'].apply(list).reset_index()\n",
        "\n",
        "print(result_grouped)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dGHLHif7X1B"
      },
      "outputs": [],
      "source": [
        "new = choro[['title_accent_fold','countries','lang']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiGJeywf7X4W"
      },
      "outputs": [],
      "source": [
        "languagesss = ['english', 'japanese', 'deutch', 'italian', 'finnish', 'french', 'spanish', 'portuguese', 'kolsch', 'zxx', 'danish']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvqvSB3y7YG3"
      },
      "outputs": [],
      "source": [
        "def replace_random(value):\n",
        "    if value == '':\n",
        "        return random.choice(languagesss)\n",
        "    else:\n",
        "        return value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLD8tOeo7f8s"
      },
      "outputs": [],
      "source": [
        "new['lang']= new['lang'].apply(replace_random)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcURvGk77hRv"
      },
      "outputs": [],
      "source": [
        "dicoo = {}\n",
        "for c in coun:\n",
        "    dicoo[c] = []\n",
        "keys_ = dicoo.keys()\n",
        "for index, row in new.iterrows():\n",
        "  for c in row.countries:\n",
        "    if c in keys_:\n",
        "      dicoo[c].append((row.title_accent_fold,row.lang))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_N1ds097hUg"
      },
      "outputs": [],
      "source": [
        "country_df = pd.DataFrame()\n",
        "country_df ['Country'] = coun\n",
        "country_df['songs'] = [dicoo[c] for c in coun]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teKXlEmK7hYA"
      },
      "outputs": [],
      "source": [
        "all_language = np.unique(new_choro.lang.values)\n",
        "dico_lang = {}\n",
        "for c in all_language:\n",
        "    dicoo[c] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0na0T0rf7say"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "dico_coun_  = {}\n",
        "for c in coun:\n",
        "    dico_coun_[c] = []\n",
        "for index, row in country_df.iterrows():\n",
        "  langs = []\n",
        "  for c in row.songs:\n",
        "      langs.append(c[1])\n",
        "  xx = collections.Counter(langs)\n",
        "  dico_coun_[row.Country] = sorted(xx.items(), key=lambda x:x[1],reverse= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NcDROaG7sdv"
      },
      "outputs": [],
      "source": [
        "country_df['languageRanking'] = [dico_coun_[c] for c in coun]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv4sAyiy7sgl"
      },
      "outputs": [],
      "source": [
        "country_df.to_json('country_data.json',orient='records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTJsjjQj74hS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json('country_data.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPNxEdap78Bq"
      },
      "outputs": [],
      "source": [
        "english = []\n",
        "japanese = []\n",
        "deutch = []\n",
        "italian = []\n",
        "finnish = []\n",
        "french = []\n",
        "spanish = []\n",
        "portuguese = []\n",
        "kolsch = []\n",
        "zxx = []\n",
        "danish = []\n",
        "for i in range(len(df)):\n",
        "    english.append(df.languageRanking[i][0][1])\n",
        "    french.append(df.languageRanking[i][1][1])\n",
        "    deutch.append(df.languageRanking[i][2][1])\n",
        "    spanish.append(df.languageRanking[i][3][1])\n",
        "    kolsch.append(df.languageRanking[i][4][1])\n",
        "    japanese.append(df.languageRanking[i][5][1])\n",
        "    zxx.append(df.languageRanking[i][6][1])\n",
        "    finnish.append(df.languageRanking[i][7][1])\n",
        "    portuguese.append(df.languageRanking[i][8][1])\n",
        "    italian.append(df.languageRanking[i][9][1])\n",
        "    danish.append(df.languageRanking[i][10][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d7s9gSN7-DI"
      },
      "outputs": [],
      "source": [
        "df['english'] = english\n",
        "df['french ']= french\n",
        "df['deutch ']= deutch\n",
        "df['spanish ']= spanish\n",
        "df['kolsch'] = kolsch\n",
        "df['japanese'] = japanese\n",
        "df['zxx ']= zxx\n",
        "df['finnish ']=finnish\n",
        "df['portuguese'] = portuguese\n",
        "df['italian ']= italian\n",
        "df['danish'] = danish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_json('country.json',orient='records')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
